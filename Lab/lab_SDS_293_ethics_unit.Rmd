---
title: "Lab# - Fairness"
author: "SDS 293: Modeling for Machine Learning"
date: "12/16/2021"
output: html_document
---

## Before we begin...

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = FALSE)

```


```{r libraries}

library(reticulate) # provides a comprehensive set of tools for interoperability between Python and R.

```

```{r}
# configure python
reticulate::py_config() # Double check that reticulate is actually using your new conda env.
reticulate::py_install("sklearn", pip = TRUE) # force install with pip. sklearn wasn't coming up via anaconda.
reticulate::py_install("matplotlib")
reticulate::py_install("keras")

# setting up the Python environment and bringing in the required Python packages is important.
# It will probably take a little while so be patient and try to avoid accidentally running
# this chunk of code.

# common trouble shooting:
# if you're missing a package then try adding its name in an additonal line of py_install
# if py_install isn't working then try adding the pip = TRUE argument to try installing
# the library through pip rather than anaconda
```

## Goals for this lab
Compute trust scores for the model's predictions of penguin species.
Practice working with Python in an R environment.


```{python}
import numpy as np
import trustscore # you need to have trustscore.py in the same folder as this .Rmd file to import it
import trustscore_evaluation  # you need to have trustscore_evaluation.py in the same folder as this .Rmd file to import it
import numpy as np
import matplotlib.pyplot as plt
import keras

# heads up! there might be some scary errors about "dlerror: cudart64_110.dll not found". It's just a warning and you can ignore it.

```

```{python}
# Import the default dataset from SK learn: the wine dataset and the classifying
# United States Post Office digits / handwriting.
from sklearn import datasets
wine = datasets.load_wine()
X_wine = wine.data
y_wine = wine.target
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

datasets = [(X_wine, y_wine), (X_digits, y_digits)]
dataset_names = ["Wine", "Digits"]

```

```{python}

from sklearn.linear_model import LogisticRegression
# Train logistic regression on digits.
model = LogisticRegression()
model.fit(X_digits[:1300], y_digits[:1300])
# Get outputs on testing set.
y_pred = model.predict(X_digits[1300:])
# Initialize trust score.
trust_model = trustscore.TrustScore()
trust_model.fit(X_digits[:1300], y_digits[:1300])
# Compute trusts score, given (unlabeled) testing examples and (hard) model predictions.
trust_score = trust_model.get_score(X_digits[1300:], y_pred)

print(trust_score) # prints the trust scores for each point in the inputted dataset

# type(trust_score) # this trust_score dataframe has a class of <class 'numpy.ndarray'>

```

### Check for understanding:
Why are the trust scores generally between 0 and 2?'


Are there any negative trust scores? Why do you think that is?
(Hint: think about the independent functions that create each trust score's numeric value)


Do a quick skim and find a trust score that is an outliar compared to the other numbers. Describe that trust score's numeric value through its context in the model as a whole.



```{r accessing Python objects in R}
# extra coding skills (optional)

# You can access variables created in the Python environment in the R studio Python 
# environment but not the R environment, unless you define the objects within an
# R chunk.
# we can access to objects created within Python chunks from R using the py object (e.g. py$x would access an x variable created within Python from R).

class(py$trust_score) # should be an array 

# create an object in R by creating an R variable in an R code chunk
trust_score_as_an_R_data_structure <- py$trust_score

```

```{python}
# Creates graphs about how correct the preduction is for the wine dataset via logistic regression

# this will produce matplotlib graphs below the code chunk, like how ggplot does.
# it will also produce some scary looking but generally ignoreable text about
# reaching the total number of iterations.
# According to Stack Overflow, this means
# "On the other hand, if the error is varying noticeably (even if the error is 
# relatively small [like in your case the score was good], but rather the differences 
# between the errors per iteration is greater than some tolerance) then we say the 
# algorithm did not converge." - https://stackoverflow.com/a/62659927

for dataset_idx,  dataset_name in enumerate(dataset_names):
  extra_plot_title = dataset_name + " | Logistic Regression | Predict Correct"
  percentile_levels = [0 + 0.5 * i for i in range(200)]
  signal_names = ["Trust Score"]
  signals = [trustscore.TrustScore()]
  trainer = trustscore_evaluation.run_logistic
  X, y = datasets[dataset_idx]
  trustscore_evaluation.run_precision_recall_experiment_general(X,
                                                                y,
                                                                n_repeats=10,
                                                                percentile_levels=percentile_levels,
                                                                trainer=trainer,
                                                                signal_names=signal_names,
                                                                signals=signals,
                                                                extra_plot_title=extra_plot_title,
                                                                skip_print=True,
                                                                predict_when_correct=True)

```


```{python}

# Graph about the points in the Wine dataset that were classified incorrectly, broken down by the model confidence and the trust score

for dataset_idx,  dataset_name in enumerate(dataset_names):
  extra_plot_title = dataset_name + " | Logistic Regression | Predict Incorrect"
  percentile_levels = [70 + 0.5 * i for i in range(60)]
  signal_names = ["Trust Score"]
  signals = [trustscore.TrustScore()]
  trainer = trustscore_evaluation.run_logistic
  X, y = datasets[dataset_idx]
  trustscore_evaluation.run_precision_recall_experiment_general(X,
                                                                y,
                                                                n_repeats=10,
                                                                percentile_levels=percentile_levels,
                                                                trainer=trainer,
                                                                signal_names=signal_names,
                                                                signals=signals,
                                                                extra_plot_title=extra_plot_title,
                                                                skip_print=True)


```
## Going a step further
Try another model from the trustscore_evaluation.py file, like run_random_forest() or run_simple_NN().


## Submitting this lab

Write about a situation in a past lab or real-life where you wish you could have used trust scores or a similar metric to check your model's predictions. If you cannot think of a situation, then detail why you don't think trust scores were necessary for that model.


### Resources
Trust Scores functions from the Google Scholars paper: https://github.com/google/TrustScore
To Trust Or Not To Trust A Classifier paper by Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta: https://arxiv.org/abs/1805.11783
Reticulate documentation: https://rstudio.github.io/reticulate/
Stack Overflow failed to converge error answer, by 
Yahya and Peter Mortensen: https://stackoverflow.com/a/62659927

